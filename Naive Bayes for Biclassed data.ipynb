{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, making classification model for the Biclassed dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'bank-full.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ca029bd3283b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbank_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bank-full.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'bank-full.csv' does not exist"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), )\n",
    "bank_data = pd.read_csv('bank-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bank_data[bank_data.columns[:-1]]\n",
    "y = bank_data[bank_data.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(bank_data, hue = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, i wrote a function which will split data into two seperate dataframes. One is training dataframe and testing dataframe. On the basis of test split percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_split = 0.5, rand_state = 10):\n",
    "    mask = np.random.rand(x.shape[0]) < test_split\n",
    "    x_test = x[mask]\n",
    "    y_test = y[mask]\n",
    "    x_train = x[~mask]\n",
    "    y_train = y[~mask]\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, test_split = 0.5, rand_state = 10):\n",
    "    mask = np.random.rand(x.shape[0]) < test_split\n",
    "    x_test = x[mask]\n",
    "    x_train = x[~mask]\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(bank_data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in x.columns.values:\n",
    "    print (\"Column {}\".format(column))\n",
    "    print (x[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classification is a classification algorithm. Where we are using probability of multiple features to predict the class of any probability. \n",
    "Naive bayes classifier is based on Naive Bayes Theorem. That theorem is a conditional probability which is gives the probability of any value, Where one even is already happen.\n",
    "\n",
    "Posterior probability is the probability which comes after performing the operation or we can say posterior probability is the resultant probability. \n",
    "\n",
    "Prior probability is the intial probability for a every particular values. \n",
    "\n",
    "So,  Formula is:\n",
    "$$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$$\n",
    "Where, <br>\n",
    "$P(A|B)$ is the posterior probability. <br>\n",
    "$P(A)$ is the prior probability of the class A. <br>\n",
    "$P(B)$ is the prior probability of the class B. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we have multiple features or we can say we have multiple values of B.\n",
    "So, in this case, we always assume that all the features are independent.<br>\n",
    "The formula is going to modify as.\n",
    "\n",
    "$$P(class = Y| features...) = \\frac{P(features ...| class = Y) P(class = Y)}{P(features...|class = Y)P(class = Y) + P(features... | class N) P(class = N)}$$\n",
    "<br><br>\n",
    "$$P\\big(class = Y| age = 40\\cap job = admin\\cap matiral = married \\cap education = secondary \\cap default = yes \\cap balance = 2000 \\cap day = 4\\cap month = jun\\cap duration = 261\\cap campaign = 4 \\cap pday = 151\\cap previous = 5\\cap poutcome = other\\big) = \\frac{P\\big(age = 40\\cap job = admin\\cap matiral = married \\cap education = secondary \\cap default = yes \\cap balance = 2000 \\cap day = 4\\cap month = jun\\cap duration = 261\\cap campaign = 4 \\cap pday = 151\\cap previous = 5\\cap poutcome = other | class = Y\\big) P(class = Y)}{P\\big(age = 40\\cap job = admin\\cap matiral = married \\cap education = secondary \\cap default = yes \\cap balance = 2000 \\cap day = 4\\cap month = jun\\cap duration = 261\\cap campaign = 4 \\cap pday = 151\\cap previous = 5\\cap poutcome = other | class = Y\\big) P(class = Y) + P\\big(age = 40\\cap job = admin\\cap matiral = married \\cap education = secondary \\cap default = yes \\cap balance = 2000 \\cap day = 4\\cap month = jun\\cap duration = 261\\cap campaign = 4 \\cap pday = 151\\cap previous = 5\\cap poutcome = other | class = N\\big) P(class = N)}$$\n",
    "\n",
    "taking some features \n",
    "\n",
    "$$P(class = Y | age = 40\\cap job = admin) = \\frac{P(age = 40 | class = Y) P(job = admin|class = Y) P(class = Y)}{P(age = 40 | class = Y) P(job = admin|class = Y) P(class = Y) + P(age = 40 | class = N) P(job = admin|class = N) P(class = N)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier (object):\n",
    "    \n",
    "    def __init__(self, data, target):\n",
    "        \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.weights = {}\n",
    "        \n",
    "    def estimate_mean_std(self, a, total_sample_size = 1000, each_sample_size = 10):\n",
    "        \n",
    "        '''\n",
    "        Estimating natural paramter of normal distribution\n",
    "        '''\n",
    "        \n",
    "        ele_list = np.array(a)\n",
    "        random_sample = np.random.choice(ele_list, size = (total_sample_size, each_sample_size))\n",
    "        \n",
    "        ## Estimating the value of Mean\n",
    "        estimate_mean = np.mean(random_sample, axis = 1)\n",
    "        best_estimate_mean = np.mean(estimate_mean)\n",
    "        \n",
    "        ## Taking Best estimate of standard deviation\n",
    "        estimate_std = np.std(random_sample, axis = 1)\n",
    "        best_estimate_std = np.mean(estimate_std)\n",
    "        \n",
    "        return {'mu':best_estimate_mean, 'sigma' :best_estimate_std}\n",
    "    \n",
    "    def estimate_bino_pValue (self, a, total_sample_size = 1000, each_sample_size = 10):\n",
    "        \n",
    "        '''\n",
    "        Estimating natural parameter of binomial distribution\n",
    "        '''\n",
    "        uniques = np.unique(a)\n",
    "        random_sample = np.random.choice(np.array(a), size = (total_sample_size, each_sample_size))\n",
    "        prob_dict = {}\n",
    "        for uni in uniques:\n",
    "            val = []\n",
    "            for sample in random_sample:\n",
    "                val.append(np.count_nonzero(uni == sample) / each_sample_size)\n",
    "            prob_dict[uni] = np.mean(val)\n",
    "        return prob_dict\n",
    "    \n",
    "    def normal_pdf(self, x, mu, sigma):\n",
    "        \n",
    "        '''\n",
    "        Proabability distribution function for Normal Distribution\n",
    "        '''\n",
    "        scale_parm = 1 / (np.sqrt(2 * np.pi) * sigma)\n",
    "        shift = np.e ** -(((x - mu) ** 2) / (2 * sigma ** 2))\n",
    "        return scale_parm * shift\n",
    "    \n",
    "    def calculate_class_prob(self):\n",
    "        '''\n",
    "        Calculating class probability for every class\n",
    "        '''\n",
    "        self.unique_classes = np.unique(self.data[self.target])\n",
    "        self.class_prob = {}\n",
    "        for class_val in self.unique_classes:\n",
    "            self.class_prob[class_val] = np.count_nonzero(self.data[self.target] == class_val) / self.data.shape[0]\n",
    "\n",
    "    def calculate_class_based_feature_prob(self, feature):\n",
    "        \n",
    "        '''\n",
    "        Calculating probability for particular feature on the basis of resultant classes\n",
    "        '''\n",
    "        val = {}\n",
    "        for classes in self.unique_classes:\n",
    "            mask = self.data[self.target] == classes\n",
    "            feature_now = feature[mask]\n",
    "            val[classes] = self.estimate_bino_pValue(feature_now)\n",
    "        return val\n",
    "    \n",
    "    def calculate_class_based_normal_feature(self, feature):\n",
    "        val = {}\n",
    "        for classes in self.unique_classes:\n",
    "            mask = self.data[self.target] == classes\n",
    "            feature_now = feature[mask]\n",
    "            val[classes] = self.estimate_mean_std(feature_now)\n",
    "        return val\n",
    "    \n",
    "    \n",
    "    def predict(self, feature):\n",
    "        \n",
    "        pred = {}\n",
    "        pred_class_sum = 0\n",
    "        pred_class = None\n",
    "        total = 0\n",
    "        for classes in self.unique_classes:\n",
    "            pred[classes] = {}\n",
    "            sumVal = 1\n",
    "            # Each feature probability......\n",
    "            for key, val in feature.items():\n",
    "                if key in self.weights:\n",
    "                    \n",
    "                    if 'mu' in self.weights[key][classes].keys():\n",
    "                        pred[classes][key] = self.normal_pdf(val, self.weights[key][classes]['mu'], self.weights[key][classes]['sigma'])\n",
    "                        sumVal *= pred[classes][key]\n",
    "                    else:\n",
    "                        pred[classes][key] = self.weights[key][classes][val]\n",
    "                        sumVal *= pred[classes][key]\n",
    "            pred[classes]['sum'] = sumVal\n",
    "            total += sumVal\n",
    "        \n",
    "        posterior_prob = {}\n",
    "        posterior = 0\n",
    "        classes = 0\n",
    "        for i in self.unique_classes:\n",
    "            posterior_prob[i] = pred[i]['sum'] * self.class_prob[i] / total\n",
    "            if (posterior_prob[i] > posterior):\n",
    "                posterior = posterior_prob[i]\n",
    "                classes = i\n",
    "        return classes\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        '''\n",
    "        Fitting data for every feature\n",
    "        '''\n",
    "        self.calculate_class_prob()\n",
    "        self.weights['age'] = self.calculate_class_based_normal_feature(self.data['age'])\n",
    "        self.weights['job'] = self.calculate_class_based_feature_prob(self.data['job'])\n",
    "        self.weights['marital'] = self.calculate_class_based_feature_prob(self.data['marital'])\n",
    "        self.weights['default'] = self.calculate_class_based_feature_prob(self.data['default'])\n",
    "        self.weights['balance'] = self.calculate_class_based_normal_feature(self.data['balance'])\n",
    "        self.weights['housing'] = self.calculate_class_based_feature_prob(self.data['housing'])\n",
    "        self.weights['loan'] = self.calculate_class_based_feature_prob(self.data['loan'])\n",
    "        self.weights['contact'] = self.calculate_class_based_feature_prob(self.data['contact'])\n",
    "        self.weights['day'] = self.calculate_class_based_feature_prob(self.data['day'])\n",
    "        self.weights['month'] = self.calculate_class_based_feature_prob(self.data['month'])\n",
    "        self.weights['duration'] = self.calculate_class_based_normal_feature(self.data['duration'])\n",
    "        self.weights['campaign'] = self.calculate_class_based_normal_feature(self.data['campaign'])\n",
    "        self.weights['pdays'] = self.calculate_class_based_normal_feature(self.data['pdays'])\n",
    "        self.weights['previous'] = self.calculate_class_based_normal_feature(self.data['previous'])\n",
    "        self.weights['poutcome'] = self.calculate_class_based_feature_prob(self.data['poutcome'])\n",
    "        \n",
    "    def predict_batch(self, data):\n",
    "        \n",
    "        classes = data.apply(self.predict,axis = 1)\n",
    "        return classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesClassifier(x_train, 'y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_class_prob()\n",
    "nb.class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.weights['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict(x_test.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = nb.predict_batch(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score_Matrix(object):\n",
    "    \n",
    "    '''\n",
    "    This class has all score matrix by which we can check efficiency of our algorithm on given dataset.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, y_actual, y_pred):\n",
    "        self.y_actual = y_actual\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        \n",
    "    def confusion_matrix(self):\n",
    "        self.data = pd.crosstab(self.y_actual, self.y_pred, rownames= ['actual'], colnames = ['predicted'], margins = True)\n",
    "        return self.data\n",
    "        \n",
    "    def recall(self):\n",
    "        '''\n",
    "        data:\n",
    "            Dataframe of confusion matrix\n",
    "        '''\n",
    "        sumVal = 0\n",
    "        self.confusion_matrix()\n",
    "        for i in self.data.columns[:-1]:\n",
    "            sumVal += (self.data[i][i] / self.data['All'][i])\n",
    "        return np.round(sumVal / (self.data.shape[0] - 1), 3)\n",
    "    \n",
    "    def check_accuracy(self):\n",
    "        sumVal = 0\n",
    "        self.confusion_matrix()\n",
    "        for i in self.data.columns[:-1]:\n",
    "            sumVal += self.data[i][i]\n",
    "        return np.round(sumVal / self.data['All']['All'], 3)\n",
    "    \n",
    "    def precision(self):\n",
    "        sumVal = 0\n",
    "        self.confusion_matrix()\n",
    "        for i in self.data.columns[:-1]:\n",
    "            sumVal += (self.data[i][i] / self.data[i]['All'])\n",
    "        avg = sumVal / (self.data.shape[0]- 1)\n",
    "        return np.round(avg,3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = Score_Matrix(x_test['y'], pred_classes)ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Accuracy on test data: {}'.format(sm.check_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Recall value on test data: {}'.format(sm.recall()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-34a45dcb8754>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "sm.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
