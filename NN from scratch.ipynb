{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layer_count = 1\n",
    "        self.parameters = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def W_initializer(self, shape):\n",
    "        \n",
    "        '''\n",
    "        This function will return a matrix of normally distributed random values of 0 mean and 1 std. \n",
    "        Parameters:\n",
    "        shape: List, Tuple\n",
    "            Shape will be a tuple of two values input shape and output shape values. \n",
    "        '''\n",
    "        return np.random.randn(shape[0], shape[1])\n",
    "    \n",
    "    def Bias_initializer(self, shape):\n",
    "        '''\n",
    "        This function will return a vector of normally distributed random values of 0 mean and 1 std. \n",
    "        parameters\n",
    "        shape: int\n",
    "            Output Vector size\n",
    "        '''\n",
    "        return np.random.normal(size = shape)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        This function will return sigmoid activated value. \n",
    "        parameters\n",
    "        x:\n",
    "            x will be the power value, a matrix multiplication of theta vector and feature vector. \n",
    "        '''\n",
    "        return 1 / (1 * np.exp(- x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        '''\n",
    "        ReLU is an activation function\n",
    "        '''\n",
    "        return np.maximum(x, np.zeros(x.shape))\n",
    "    \n",
    "    def soft_max(self, x):\n",
    "        \n",
    "        '''\n",
    "        Soft max is an activation function vector. \n",
    "        parameters\n",
    "        x: np.array\n",
    "            x will be the power values\n",
    "        '''\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis = 1)[:, None]\n",
    "    \n",
    "    def add_layer(self, in_shape, out_shape, activation = 'relu'):\n",
    "        \n",
    "        '''\n",
    "        This funtion will add a layer into the neural network. Assigning all the parameters to the neural network. \n",
    "        Also assigning the activation function into the paramters. \n",
    "        '''\n",
    "        \n",
    "        self.parameters['layer: ' + str(self.layer_count)] = {'W': self.W_initializer((in_shape, out_shape)), 'b': np.ones((out_shape))}\n",
    "        \n",
    "        if (activation == 'relu'):\n",
    "            self.parameters['layer: ' + str(self.layer_count)]['Activation'] = self.relu\n",
    "        elif (activation == 'sigmoid'):\n",
    "            self.parameters['layer: ' + str(self.layer_count)]['Activation'] = self.sigmoid\n",
    "        elif (activation == 'soft_max'):\n",
    "            self.parameters['layer: ' + str(self.layer_count)]['Activation'] = self.soft_max\n",
    "            \n",
    "        self.layer_count += 1\n",
    "    \n",
    "    def dense_layer(self, x, parameters):\n",
    "        \n",
    "        '''\n",
    "        Dense Layer is the matrix multiplication of Weight matrix and feature vector then adding the bias in it. \n",
    "        Activation function will call after operation. \n",
    "        '''\n",
    "        W = parameters['W']\n",
    "        bias = parameters['b']\n",
    "        activation = parameters['Activation']\n",
    "        Z = np.matmul(x, W) + bias\n",
    "        return activation(Z)\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \n",
    "        '''\n",
    "        Forward Pass\n",
    "        '''\n",
    "        result = x\n",
    "        self.results['A0'] = result\n",
    "        for i in range(1, self.layer_count):\n",
    "            result = self.dense_layer(result, self.parameters['layer: ' + str(i)])\n",
    "            self.results['A' + str(i)] = result\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def del_forward_pass(self):\n",
    "        '''\n",
    "        This function will calculate the derivatives for the forward pass. \n",
    "        '''\n",
    "        \n",
    "        dev_forpass = {}\n",
    "        for i in range(1, self.layer_count):\n",
    "            act = self.parameters['layer: ' + str(i)]['Activation'] ## Taking activation for calculating different derivatives\n",
    "            result = self.results['A' + str(i - 1)]\n",
    "            if (act == self.relu):\n",
    "                dev_forpass['layer: ' + str(i)] = {'dW': result.T, 'db': result.T}\n",
    "            elif (act == self.sigmoid):\n",
    "                sig = self.sigmoid(result.T)\n",
    "                dev_forpass['layer: ' + str(i)] = {'dW': ((sig * (1 - sig)) * result.T), 'db': (1 - sig)}\n",
    "\n",
    "        return dev_forpass\n",
    "    \n",
    "    def del_backward_pass(self, in_shape, error):\n",
    "        \n",
    "        '''\n",
    "        In backward propogation we have to calculate all the values \n",
    "        \n",
    "        '''\n",
    "        dev_backpass = {}\n",
    "        theta_mul = np.ones((in_shape, 1)) * error ## Last Layer dL / dA\n",
    "        \n",
    "        for i in range(self.layer_count - 1, 1, -1):\n",
    "\n",
    "            dev_backpass['layer: '+ str(i)] = {'dW': theta_mul, 'db': 1} ## Storing del Activations \n",
    "            theta_val = self.parameters['layer: ' + str(i)] ## Taking thetas of l + 1 layer\n",
    "            theta_mul =  theta_mul @ theta_val['W'].T ## Calculating del Activation \n",
    "\n",
    "        dev_backpass['layer: '+ str(i - 1)] = {'dW': theta_mul, 'db': 1}\n",
    "        return dev_backpass\n",
    "    \n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        return self.forward_pass(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.forward_pass(x))\n",
    "    \n",
    "    def GradientDescentOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        loss_dir = []\n",
    "        for _ in range(epoch):\n",
    "            \n",
    "            result = self.forward_pass(x)\n",
    "            del_loss = self.biclass_cross_entropy(x, y)\n",
    "            del_forward = self.del_forward_pass()\n",
    "            del_backward = self.del_backward_pass(x.shape[0], del_loss)\n",
    "            \n",
    "            for i in range(1, self.layer_count):\n",
    "                self.parameters['layer: ' + str(i)]['W'] -= alpha * (del_forward['layer: ' + str(i)]['dW'] @ del_backward['layer: ' + str(i)]['dW'])\n",
    "#                 self.parameters['layer: ' + str(i)]['b'] -= alpha * (del_forward['layer:' + str(i)]['db'] @ del_backward['layer: ' + str(i)]['db'])\n",
    "\n",
    "            loss = self.Biclass_Loss(result, y)\n",
    "            print ('Loss: ', loss)\n",
    "            loss_dir.append(loss)\n",
    "        return loss_dir\n",
    "    \n",
    "    def AdamOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def AdagradeOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "        \n",
    "    def RMSpropOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def MomentumUpdateOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def NesterovMomentumOptimizer(self, x, y, alpha, epoch = 100):\n",
    "        pass\n",
    "    \n",
    "    def softmax_cross_entropy(self, logits, labels):\n",
    "        return (- np.mean( labels - np.log(self.soft_max(logits))))\n",
    "    \n",
    "    def Biclass_Loss(self, x, y):\n",
    "        sig = self.sigmoid(x)\n",
    "        return -np.mean( (y * np.log(sig)) + ((1 - y) * np.log(1 - sig)))\n",
    "    \n",
    "    def multiclass_Loss(self, x, y):\n",
    "        return -np.mean(y * np.log(self.soft_max(x)))\n",
    "    \n",
    "    def biclass_cross_entropy(self, x, y):\n",
    "        return -np.mean((np.log(self.sigmoid(x)) - y) * x)\n",
    "    \n",
    "    def fit(self, x, y, learning_rate, optimizer = 'grad_dst', epoch = 100):\n",
    "        \n",
    "        if (optimizer == 'grad_dst'):\n",
    "            self.GradientDescentOptimizer(x, y, learning_rate, epoch)\n",
    "        elif (optimizer == 'adam'):\n",
    "            pass\n",
    "        elif (optimizer == 'rmsprop'):\n",
    "            pass\n",
    "        elif(optimizer == 'adagrade'):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add_layer(3, 3, 'sigmoid')\n",
    "nn.add_layer(3, 2, 'sigmoid')\n",
    "nn.add_layer(2, 1, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer: 1': {'W': array([[-0.20264511,  1.45125769,  1.50001282],\n",
       "         [ 0.55321011,  0.80195992,  0.00647126],\n",
       "         [ 0.87300533,  0.94163917, -0.837597  ]]),\n",
       "  'b': array([1., 1., 1.]),\n",
       "  'Activation': <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x0000027638A34550>>},\n",
       " 'layer: 2': {'W': array([[-0.75136743,  0.78957496],\n",
       "         [-1.9245775 , -0.93031235],\n",
       "         [-0.81521169,  0.40858933]]),\n",
       "  'b': array([1., 1.]),\n",
       "  'Activation': <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x0000027638A34550>>},\n",
       " 'layer: 3': {'W': array([[ 1.29944781],\n",
       "         [-0.09488551]]),\n",
       "  'b': array([1.]),\n",
       "  'Activation': <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x0000027638A34550>>}}"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.normal(5, 2, (128, 3))\n",
    "y = np.random.randint(0, 2, (128, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  nan\n"
     ]
    }
   ],
   "source": [
    "nn.fit(data, y, 0.01, epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = nn.del_forward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "backwar = nn.del_backward_pass(128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_layer = {}\n",
    "\n",
    "for i in range(1, nn.layer_count):\n",
    "    del_layer['layer: ' + str(i)] = forward['layer: ' + str(i)]['dW'] @ backwar['layer: ' + str(i)]['dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 128)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.del_forward_pass()['layer: 2']['dW'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward_pass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.69626228,  1.79499291,  8.23735869],\n",
       "       [ 7.35265278,  6.02810883,  3.58616242],\n",
       "       [ 1.66855587,  5.14228062,  3.91397677],\n",
       "       [ 5.65037415,  5.48278021,  4.25544984],\n",
       "       [ 0.89814364,  5.0109521 ,  5.77441157],\n",
       "       [ 7.14154712,  3.30396138,  5.63214847],\n",
       "       [ 5.42251669,  3.55184817,  3.46878872],\n",
       "       [ 6.64108641,  1.20216384,  4.3881687 ],\n",
       "       [ 4.62572031,  2.18575557,  6.54451518],\n",
       "       [ 2.07476679,  3.96828345,  3.93470106],\n",
       "       [ 3.88121598,  4.92247223,  5.78725813],\n",
       "       [ 7.65191359,  5.56795074,  3.72692372],\n",
       "       [ 7.36929209,  7.27418525,  5.20288485],\n",
       "       [ 6.17384108,  2.41006097,  3.18356742],\n",
       "       [ 3.69989098,  2.86958069,  6.2347652 ],\n",
       "       [ 3.90841427,  4.36662797,  6.67352831],\n",
       "       [ 4.21107696,  2.51663732,  4.58297365],\n",
       "       [ 1.87132718,  4.13050497,  7.99173897],\n",
       "       [ 5.85417263,  2.06052481,  4.13983191],\n",
       "       [ 3.15168327,  5.65536783,  6.07725891],\n",
       "       [ 4.34316957,  4.73211736,  6.35759134],\n",
       "       [ 6.48126613,  2.6391104 ,  6.14586225],\n",
       "       [ 3.60865578,  5.81948805,  3.28035492],\n",
       "       [ 2.39017581,  4.15931951,  3.33492061],\n",
       "       [ 6.12016999,  4.00902425,  3.84252489],\n",
       "       [ 4.92725795,  3.78904168,  4.51525683],\n",
       "       [ 4.40809024,  6.19504667,  8.46661122],\n",
       "       [ 5.47194404,  3.20512946,  7.34786668],\n",
       "       [ 6.74284198,  6.17561747,  1.71158512],\n",
       "       [ 4.76137658,  2.86962814,  5.15917337],\n",
       "       [ 5.11035723,  0.23131873,  6.54292601],\n",
       "       [ 4.82005135,  5.63984185,  4.95757104],\n",
       "       [ 3.04691233,  6.70480711,  3.37858666],\n",
       "       [ 4.44025826,  7.23353282,  5.02463624],\n",
       "       [ 5.2585074 ,  4.59290817,  6.54284859],\n",
       "       [ 7.81519574,  8.58629511,  6.12390888],\n",
       "       [ 7.30486508,  5.0759119 ,  6.63641502],\n",
       "       [ 4.2048012 ,  8.69081705,  5.69942858],\n",
       "       [ 6.11783895,  7.89743799,  1.80876609],\n",
       "       [ 5.34095834,  6.76049826,  8.74701789],\n",
       "       [ 4.32918773,  4.97648661,  3.04872414],\n",
       "       [ 2.42930365, -0.3350013 ,  1.77989701],\n",
       "       [ 5.40028357,  5.8611788 ,  5.6126859 ],\n",
       "       [ 5.58680025,  2.46473728,  3.80183597],\n",
       "       [ 7.33053129,  7.8328458 ,  7.02506589],\n",
       "       [ 3.90702213,  4.21689167,  5.9722708 ],\n",
       "       [10.32750113,  5.92574379,  5.53493916],\n",
       "       [ 4.42893713,  7.40656957,  8.49386481],\n",
       "       [ 4.96026641,  1.22510638,  6.9717258 ],\n",
       "       [ 5.3837471 ,  6.74260562,  4.49429443],\n",
       "       [ 5.81422715,  2.96402518,  4.13450145],\n",
       "       [ 5.33764061,  3.766453  ,  3.39215607],\n",
       "       [ 4.35238919,  8.88365415,  3.88470195],\n",
       "       [ 7.90889036,  5.770259  ,  8.49532394],\n",
       "       [ 5.0080746 ,  4.12935223,  1.0346412 ],\n",
       "       [ 3.1974743 ,  4.66121658,  2.64920738],\n",
       "       [ 5.05082603,  1.00056432,  5.93833049],\n",
       "       [ 6.60882859,  2.18929827,  4.43554541],\n",
       "       [ 3.82690433,  0.85798414,  0.76228099],\n",
       "       [ 4.80621588,  4.59467391,  6.57281238],\n",
       "       [ 7.75627182,  2.66534212,  6.42830333],\n",
       "       [ 1.34579972,  4.57702182,  5.16395763],\n",
       "       [ 3.22826527,  5.5161297 ,  3.48009797],\n",
       "       [ 4.27631301,  6.48199278,  5.08701127],\n",
       "       [ 4.63051148,  9.852045  ,  3.5504436 ],\n",
       "       [ 4.20955046,  5.44999277,  4.28874578],\n",
       "       [ 7.22743868,  5.01740916,  6.90082144],\n",
       "       [ 2.13391102,  8.54289784,  2.90021388],\n",
       "       [ 7.05988558,  9.91944035,  4.15416511],\n",
       "       [ 7.56550342,  3.9280528 ,  4.12687358],\n",
       "       [ 6.3824488 ,  3.42161253,  6.90878201],\n",
       "       [ 4.24608335,  3.41886154,  3.00513443],\n",
       "       [ 4.06847099,  5.07482611,  2.00174418],\n",
       "       [ 6.72617728,  8.67118906,  3.85449578],\n",
       "       [ 4.89291274,  2.89307278,  5.78571022],\n",
       "       [ 4.72330757,  1.45141833,  3.13040856],\n",
       "       [ 2.57669385,  6.49516478,  6.93299114],\n",
       "       [ 2.93986043,  1.97832744,  3.34173298],\n",
       "       [ 5.1494557 ,  5.23759759,  2.94980438],\n",
       "       [ 1.8057107 ,  4.63848681,  4.7199382 ],\n",
       "       [ 2.7732419 ,  6.09696982,  4.96722907],\n",
       "       [ 1.19041978,  4.43901703,  7.13656735],\n",
       "       [ 3.75082473,  3.3618507 ,  7.1066526 ],\n",
       "       [ 0.79650011, -1.09958763,  5.62598482],\n",
       "       [ 4.14594303,  0.97765826,  4.37716676],\n",
       "       [ 7.21336167,  4.31746779,  3.43801976],\n",
       "       [ 5.57255479,  4.56810276,  4.19512174],\n",
       "       [10.57177589,  3.46233785,  4.33950405],\n",
       "       [ 4.94376468,  2.84873667,  4.27714866],\n",
       "       [ 4.41749136,  2.69104683,  2.96093908],\n",
       "       [ 6.1637741 ,  0.76728755,  4.56844082],\n",
       "       [ 2.83074005,  4.62586809,  4.83525741],\n",
       "       [ 3.66096812,  8.01238731,  1.88195866],\n",
       "       [ 3.04732866,  3.09624209,  7.49852435],\n",
       "       [ 4.26667441,  5.35433392,  7.6008587 ],\n",
       "       [ 6.65740404,  6.30712988,  3.18168953],\n",
       "       [ 3.26643465,  3.15402073,  5.60644968],\n",
       "       [ 6.78872973,  2.33418527,  4.50409105],\n",
       "       [ 3.82922453,  4.63309148,  7.03595428],\n",
       "       [ 5.86721869,  3.98482085,  4.42805273],\n",
       "       [ 2.8650114 ,  4.7442711 ,  1.18972152],\n",
       "       [ 7.24000634,  6.94062719,  3.9466356 ],\n",
       "       [ 6.9038144 ,  5.3175917 ,  4.79248239],\n",
       "       [ 5.02967812,  6.87915076,  5.52880942],\n",
       "       [ 4.77180911,  3.36429584,  8.88193542],\n",
       "       [ 2.0996768 ,  5.91644956,  4.03174763],\n",
       "       [ 6.25697801,  8.35469301,  4.07223676],\n",
       "       [ 5.36544616,  5.19995627,  5.32641963],\n",
       "       [ 7.71865722,  5.87223792,  2.15000873],\n",
       "       [ 6.19741127,  6.44749442,  7.04836209],\n",
       "       [ 4.97755244,  5.13841415,  4.98340699],\n",
       "       [ 4.25233466,  5.35132282,  5.9080799 ],\n",
       "       [ 4.89821205,  2.8600905 ,  4.74655757],\n",
       "       [ 5.21803145,  2.21160622,  3.89008132],\n",
       "       [ 2.65895591,  2.43180423,  3.16695158],\n",
       "       [ 4.67581937,  4.10294802,  5.38975731],\n",
       "       [ 3.11526996,  3.15330339,  1.84925404],\n",
       "       [ 6.91935782,  5.94704169,  1.29309016],\n",
       "       [ 3.63091047,  3.93980902,  3.80178975],\n",
       "       [ 6.04744034,  7.57672573,  5.87608526],\n",
       "       [ 6.46243154,  6.2562522 ,  1.76130687],\n",
       "       [ 5.34356775,  5.50909488,  2.16931336],\n",
       "       [ 7.02847273,  5.32745633,  3.33294696],\n",
       "       [ 6.22801038,  6.89530612,  5.27801613],\n",
       "       [ 3.32678372,  9.30988502,  8.90701765],\n",
       "       [ 4.46379155,  3.1822655 ,  6.1174381 ],\n",
       "       [ 3.37404254,  6.26780053,  6.17959732],\n",
       "       [ 7.11453464,  9.10385121,  8.04956564]])"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.results['A0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer: 1': {'W': array([[nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan]]),\n",
       "  'b': array([1., 1., 1.]),\n",
       "  'Activation': <bound method NeuralNetwork.relu of <__main__.NeuralNetwork object at 0x0000027638DBBF98>>},\n",
       " 'layer: 2': {'W': array([[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]]),\n",
       "  'b': array([1., 1.]),\n",
       "  'Activation': <bound method NeuralNetwork.relu of <__main__.NeuralNetwork object at 0x0000027638DBBF98>>},\n",
       " 'layer: 3': {'W': array([[nan],\n",
       "         [nan]]),\n",
       "  'b': array([1.]),\n",
       "  'Activation': <bound method NeuralNetwork.sigmoid of <__main__.NeuralNetwork object at 0x0000027638DBBF98>>}}"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
